# docling-rag-tutorial-pydata-2025
A repo for Building Rich RAG Systems with Docling: Unlock Information from Tables, Images, and Complex Documents https://cfp.pydata.org/virginia2025/talk/XPFPFE/ 

# Pre-work
You will need a way to do LLM inference for the `generation` step of RAG. For that you can have a local `ollama` server with a smaller model (1b, 2b, 3b) or quantized version of a larger model 
1. (Remote LLM inference) Make sure you have a Huggging Face / Replicate token if you want to perform LLM inference (look at the setup directory for more details)
2. (Local LLM inference) 
